{"/about/":{"data":{"":"This is the about page."},"title":"About"},"/blog/":{"data":{"":" RSS Feed "},"title":"Blog"},"/blog/li_ml_2017/1.%E4%BB%8B%E7%BB%8D/":{"data":{"":"hand crafed rules 基于规则的机器很僵硬，且永远不会超过制定规则的人类的智慧，因此需要让机器可以自主学习。\n机器学习本质上就是找到一个函数$f(x)$，给定一个输入x，得到一个输出y。怎样找到这个函数呢？\n可以分为3步。第一步，先人为给定一个function set即model；第二步，定义衡量一个function好坏的标准；第三步，从function set中找到一个最好的函数，即最优化问题。这就类似于把大象装进冰箱需要3个步骤一样。\n机器学习的Task：回归、分类、structure learning（如机器翻译）\nMethod：线性模型、非线性模型\nScenario：有监督学习、半监督学习、迁移学习、无监督学习、强化学习\nAlphaGO是有监督学习+强化学习"},"title":"1.介绍"},"/blog/li_ml_2017/10.cnn/":{"data":{"":"为什么CNN应用于图像？\n神经元不需要看到整副图像来发现某个模式。（如鸟类检测时，只需要看到图片的一小块区域就可以检测到鸟嘴） 相同的模式会出现在图片的不同区域中。（如鸟嘴会出现在图片的不同位置） 对图片像素进行子采样不会改变图片中的物体。 这3个理由都可以让神经网络的参数变少，因此CNN由此诞生。CNN的卷积层考虑了原因1和2，max pooling层考虑了原因3。\nFilter的大小是自己设定的，如果filter大小是3*3，那么就相当于认为模式位于3*3的图片大小内（可以被3*3的Filter检测出来）\nCNN是全连接神经网络的简化版！\n卷积相当于只激活部分输入层的神经元（对应于原因1的一小块区域）。\n卷积操作是同一个Filter对输入做卷积，因此相当于输入层连接到卷积层的权重都是共用的。\n每一层卷积层可以有多个Filter，有几个Filter相当于做完卷积后的图片有几个channel(检测出了几种模式)。\n图1 这个地方为什么model2.add(Convolution2D)(50,3,3)之后输出是50*11*11，而不是50*25*11*11，弹幕里说是把25个channel做了平均。。看下源码。。。。。"},"title":"10.CNN"},"/blog/li_ml_2017/11.why-deep/":{"data":{"":"[TOC]\n在相同的参数数量下，深的网络要比胖的网络效果要好。\n深度神经网络其实是在做模组化（Modularization），每一层相当于一个模块（或者说是子函数），浅层相当于基础的模块，深的层相当在基本模块上的基础上构建的功能更复杂的模块（子函数-\u003e主函数）。"},"title":"11.why deep"},"/blog/li_ml_2017/12.%E5%8D%8A%E7%9B%91%E7%9D%A3/":{"data":{"entropy-based-regularization#Entropy-based regularization":"一个分布的熵越大，则不确定性越高，可以将模型预测的无标签数据的概率分布的熵加入到损失函数中，即$L=\\sum_{x^{r}}C(y^r,\\hat{y}^r)+\\lambda\\sum_{x^{u}}E(y^u)$","半监督low-density#半监督+low density":"上面的半监督+生成式模型中认为无标签数所按一定的后验概率属于不同的类别（soft label），而半监督+low density则认为“非黑即白”，即无标签数据只能属于某一特定类别（hard label），其做法是先用有标签数据训练得到一个模型$f^*$，然后用该模型对无标签数据做预测，取部分预测的结果加回到有标签数据中重新训练，反复进行这个过程。需要注意的是，如果是做回归任务，那么这个方法没用，因为加入的这些数据其实是由模型$f^*$产生的，加入后重新训练得到的模型仍然是$f^*$。\n对于神经网络来说，soft label这种方法没用，原因如上。","半监督smoothness-assumption#半监督+smoothness assumption":"思想：样本的分布不是均匀的，如果x1和x2在一个高密度区域内（可以想象成聚类后的x1和x2属于同一类）是接近的，那么$\\hat{y}^1$和$\\hat{y}^2$应该是一样的。\n那么类似于Entropy-based regularization，我们也可以在损失函数中加入smooth regularization，即$L=\\sum_{x^{r}}C(y^r,\\hat{y}^r)+\\lambda S$，其中$S=\\frac{1}{2}\\sum_{i,j}w_{i,j}(y^i-y^j)^2=\\mathbf{y^T}L \\mathbf{y}$，其中i,j为一个高密度区域内的两个点，$w_{i,j}$表示两个点的相似度similarity，L为拉普拉斯矩阵。","半监督生成式模型#半监督+生成式模型":"[TOC]\n半监督+生成式模型以二分类为例，\n先初始化参数：$\\theta ={ P(C_1),P(C_2), \\mu ^{1},\\mu ^{2},\\Sigma }$\n根据模型参数$\\theta$计算无标签数据的后验概率$P_\\theta(C_1|x^u)$\n根据第2步的结果，更新模型参数 $$ P(C_1)=\\frac{N_1+\\sum_{x^u}P(C_1|x^u))}{N} $$ $$ \\mu ^1=\\frac{1}{N_1}\\sum_{x^{r}\\epsilon C_1}x^r+\\frac{1}{\\sum_{x^u}P(C_1|x^u))}\\sum_{x^u}P(C_1|x^u))x^u $$\n公式2中的第2项相当于无标签数据的加权平均，权重为后验概率。\n为什么要这样做呢？\n在有监督+生成式模型中，我们通过极大化有标签数据的似然函数$logL(\\theta)=\\sum_{x^r}logP_\\theta(x^r|\\hat{y}^r)P(\\hat{y}^r)$来求得模型参数，那么在半监督+生成式模型中，我们就要极大化有标签数据+无标签数据的似然函数$logL(\\theta)=\\sum_{x^r}logP_\\theta(x^r|\\hat{y}^r)P(\\hat{y}^r)+\\sum_{x^r}logP_\\theta(x^u)$，由于不知道$x^u$从哪一个label中生成的，因此认为2个label都有可能，所以$x^u$出现的概率等于$P_\\theta(x^u)=P_\\theta(x^u|C_1)P(C_1)+P_\\theta(x^u|C_2)P(C_2)$，但是这个式子不是convex的，因此要迭代的去解，也就是通过上面的3个步骤求解。"},"title":"12.半监督"},"/blog/li_ml_2017/13.pca/":{"data":{"pca#PCA":"","pca和svd-的区别与联系#PCA和SVD 的区别与联系":"","pca的weakness#PCA的weakness":" PCA是无监督的方式，高维空间中的两个类通过PCA降维后可能会被merge到一起，无法区分。LDA是有监督折方式。\nPCA是线性的（各主元是正交的），如下图3维空间中的S形，PCA无法把它“拉直”，只能把它“拍扁”。","pca的另一种角度#PCA的另一种角度":"[TOC]\n层次聚类类似于Hoffman树的构建。\nPCA先说公式：$Z=WX$，$X$为原始数据集，维度为n$\\times$m，n为样本数量，m为特征数量，$W$为由k个m维向量构成的矩阵（由m维降到k维），$Z$为降维后的数据，维度为n$\\times$k。\n怎样找到$W$呢， PCA的做法是：\n先找第一个m维向量w1，使数据集中的n个点投影到向量w1后的方差最大。 再找第二个m维向量w2，在保证w1和w2正交的前提下，使数据集中的n个点投影到向量w2后的方差最大。 ……一直找到第k个m维向量wk，每一个向量都要在保证与之前的向量正交的前提下，使数据集中的n个点投影到向量w2后的方差最大。 怎样找到w1,w2,…,wk呢，以w1为例， $$ Var(z_1)=\\sum_{z_1}(z_1-\\bar{z_1})^2=\\sum_x(w^1\\cdot (x-\\bar{x}))^2=(w^1)^T\\sum(x-\\bar{x})(x-\\bar{x})^Tw^1=(w^1)^TCov(x)w^1 $$ 其中，$Cov(x)$为原始数据集$X$的协方差矩阵（协方差矩阵为实对阵矩阵，其特征向量相互正交），要使上式最大，通过拉格朗日乘子法，约束条件为$w^1(w^1)^T=1$（w的模为1，单位向量），可以求得w1为协方差矩阵$Cov(x)$对应于最大的特征值的特征向量，同理，可以求出w2…wk分别为对应于第2大…第k大的特征值的特征向量。\n由于w1,w2,…,wk是相互正交的，因此降维后的数据$Z$的各特征列也是线性无关的（正交），且$Z$的协方差矩阵是对角矩阵。\nPCA和SVD 的区别与联系 区别：PCA和SVD是两个完全不同的概念。PCA是主成分分析，是一种降维方式，通过选取k个主元即k个向量，w1,w2,…,wk，将数据集降维到k维。SVD是奇异值分解，是一种矩阵分解技术。\n联系：PCA中的主成分是原始数据$X$的协方差矩阵$(x-\\bar{x})(x-\\bar{x})^T$的k个特征向量。矩阵$X$的SVD为$X=U\\Sigma V^T$中的$U$是$XX^T$的k个特征向量。因此如果将原始数据$X$中心化（各个特征列均值为0）得到$X^{’}$，对$X^{’} $做SVD，得到的$U^{’}$中的k个特征向量其实就是对原始数据$X$做PCA，得到的k个主元。\nPCA的另一种角度 通过PCA得到k个主元，数据集中每个样本可以看作是这k个主元的线性组合，每个样本线性组合的系数不同。","附#附":"PCA保留了高维 空间中的距离信息，如果两个点在高维空间中接近，那么降维后在低维空间中也是接近的。","隐因子分解#隐因子分解":"以电影评分矩阵为例，矩阵中的某些列（每列代表一个电影）和某些行（每行代表一个用户）其实是线性相关的，因此可以进行隐因子分解。\n用于NLP中，对文档和词的共现矩阵进行隐因子分解就是LSA。"},"title":"13.PCA"},"/blog/li_ml_2017/14.tsne_autoencoder/":{"data":{"cnnauto-encoder#CNN+auto-encoder":"可以用全连接层作为中间层，也可以用卷积层+池化层作中间层，那么在decoder的部分就要做de-maxpooling+ de-convuluntion","de-noising-auto-encoder#de-noising auto-encoder":"","deep-auto-encoder#Deep auto-encoder":"PCA可以看作是一个神经网络，隐层是线性的（没有激活函数），输入为$X-\\bar{X}$，输出为重构后的X,$\\hat{X}$，通过最小化重构误差得到参数。\nDeep auto-encoder是一个神经网络，也可以用于降维，输入为$X$，输出也为$X$，deep auto-encoder重点不在于得到输出，而在于得到降维后的coder，实质上就是通过神经网络的方式来降维。","t-sne#t-sne":"[TOC]\nt-snet-sne的思想是，高维空间中相近的点，经过降维之后在低维空间中的距离仍然很近；高维空间中很远的点，降维之后在低维空间中的距离也会很远，且该距离在低维空间中会被放大一些，变得更远。\nt-sne可以通过梯度下降求解。"},"title":"14.tsne_autoencoder"},"/blog/li_ml_2017/15.deep-generative-model/":{"data":{"gan#GAN":"generator VS discriminator\n先训练discriminator，再固定discriminator参数，训练generator，再固定generator参数，训练discriminator………..","pixel-rnn#Pixel RNN":"[TOC]\nPixel RNN 输入 输出 第一个像素点 第二个像素点 第一个+第二个像素点 第三个像素点 第一个+第二个+第三个像素点 第四个像素点 …… …… 其实就是RNN。因此可以通过输入图像的一部分，让RNN生成剩下的图像。","vae-variational-autoencoder#VAE (variational autoencoder)":"\nVAE为什么要这么做呢？\n**直观上的解释：**VAE和之间的auto encoder相比，多了$\\sigma _1,\\sigma _2,…$和$e_1,e_2,…$,相当于引入了随机噪声，$\\sigma _1,\\sigma _2,…$是噪声的方差，方差大小是由神经网络自己学到的，但是不能让网络完全自己决定方差大小，因为方差为0时，重构误差最小。因此需要多加入限制项（限定噪声的方差趋于1）\n理论上的解释：高斯混合模型GMM，推导没看懂。。。"},"title":"15.deep generative model"},"/blog/li_ml_2017/16.transfer/":{"data":{"model-fine-tuning#model fine-tuning":"Transfer learning 可以分成以下4种情况\nmodel fine-tuningsource data:所有人的语音数据\ntarget data:某个人的语音数据\nmodel fine-tuning的做法是，用source data 训练得到的网络的参数，作为网络的初始参数，再用target data训练网络，需要注意过拟合问题，可以固定几层参数，只微调某一层的参数。"},"title":"16.transfer"},"/blog/li_ml_2017/2.%E5%9B%9E%E5%BD%92/":{"data":{"":"介绍中说过，机器学习本质上就是找到一个函数$f(x)$，那么损失函数Loss function可以理解为函数$f(x)$的函数，它的输入为函数$f(x)$，输出为这个函数的好坏，可以表示为$L(f)$。\n只要定义了Loss function且Loss function对参数是可微的，那么就可以用梯度下降法。\n梯度下降法可以表示为：${w}’\\leftarrow w_{0}-\\eta \\frac{\\mathrm{d} L}{\\mathrm{d} w}|{w=w{0}}$\n以只有w,b两个参数的线性回归为例，损失函数的梯度为Loss function对所有参数的偏微分构成的vector，即 $$ \\triangledown L=\\begin{bmatrix} \\frac{\\partial L}{\\partial w}\\ \\ \\frac{\\partial L}{\\partial b} \\end{bmatrix} $$\n梯度下降存在的问题：\n对于非凸函数，可能得到局部最小 可能存在鞍点 可能存在平坦区域（梯度一直很小，接近于0） 对于$y=wx+b$这个线性模型来说，可以对模型进行修改，即换一个function set，可以修改为$y=w_1x^2+w_2x+b$或$y=w_1x^3+w_2x^2+w_3x+b$…，即加入x的2次项、3次项、4次项…实际上相当于提了新的feature，随着feature越来越多，模型复杂度也会越来越高，因为对于线性模型来说，模型复杂度即参数个数，也就是特征个数；但树模型的模型复杂度不会随着特征数量增多而增加，因为树模型的模型复杂度是树的深度、叶子节点个数这些。当模型复杂度过高时，训练集上的error会很低，会测试集的error会很高，即过拟合，过拟合其实就是模型学习到了这份训练集上“独有”的一些参数，就像科目二考试中看到后轮压到白线后就打满方向盘一样。\n解决过拟合的一个方法就是，扩大训练集，相当于换了个科目二场地（之前的“规律”可能就没用了）。\n另一个方法是修改Loss function，在原先的损失函数中加入正则项，使模型平滑。\n课中提到，如果不考虑宝可梦的种类，就对其CP值进行预测，是在瞎忙，因为不同类别的宝可梦，其进化模式是不同的。因为对于实际的业务场景来说，也要具体问题具体分析，根据情况建立不同的模型，例如嫌疑人预测时要区分犯罪类型。\n模型效果差可能的原因：\n预测的量本身就有随机性 还有其他隐藏因素的影响（我的理解是可以多提feature来优化） "},"title":"2.回归"},"/blog/li_ml_2017/3.%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/":{"data":{"":"现在先假设随机变量x的均值为$\\mu $，方差为$\\sigma ^{2} $，现在对其均值及方差进行估计。首先采样出N个点，若以这N个点的均值$m $作为对$\\mu $的估计值，由于$m $的期望值等于$\\mu $，那么$m $是$\\mu $的无偏估计，$m $的方差为$\\frac{\\sigma ^{2}}{N}$。 若以这N个点的方差$s^2 $作为对$\\sigma ^{2} $的估计，由于$s^2 $的期望为$\\frac{N-1}{N}\\sigma ^2$不等于$\\sigma ^{2} $，那么$s^2 $为$\\sigma ^{2} $的有偏估计，$s^2 $的方差为。。视频里没说，可以查一下。\n机器学习中所说的偏差和方差是针对模型model来说的。\n图1 偏差与方差\n上图中，位于红色靶心的$\\hat{f}$表示真正要学习的函数，${f^*}$表示模型model 在不同数据集上（即视频中说的平行宇宙） 训练得到的函数，${f^*}$的期望即模型model的偏差，表示其与真正函数$\\hat{f}$（即靶心）的偏差程度。，${f^*}$ 的离散程度即模型model的方差。\n通俗地来说，如果以击中靶心为最终目标的话，模型的偏差指的是在瞄准靶心时瞄的准不准，模型的方差指的是在瞄准之后射的准不准（射的位置和瞄的位置的偏离有多大）\n为什么模型model越简单，方差越小？ 考虑极端的情况，model（function set）中只有一个函数${f^*}=c$，那么不管数据集如何改变（位于哪个平行宇宙）,模型始终会输出同一个函数${f^*}=c$，即0方差。\n图2\n上图中，黑色的线表示真正的函数$\\hat{f}$，红色的线表示模型model在不同数据集上学习到的${f^*}$，蓝色的线表示的是所有红色线的平均。左图中的模型只有一次项，模型简单，具有高偏差、低方差；右图中的模型有5次项，模型复杂，具有低偏差、高方差。\n通俗地来说，如果用圆圈来表示模型model的覆盖范围，那么简单模型的圆圈会较小，可能覆盖不到靶心（真正的函数$\\hat{f}$），即高偏差，但是正是由于其覆盖范围小，因此其在不同数据集上训练得到的${f^*}$也会相对集中，即低方差；那么复杂模型的圆圈会较大，很可能会覆盖到靶心，即低偏差，但是正是由于其覆盖范围大，其在不同数据集上训练得到的${f^*}$也会相对分散，即高方差。\n对于复杂模型来说，由于其训练得到的${f^*}$都不一样，但是都是位于靶心附近的（低偏差），因此如果在足够多的数据集上训练出足够多的${f^*}$，最后取这些${f^*}$的平均即为“靶心”。\n对于神经网络来说，由于DNN模型复杂度高，因此所需的数据集很大，也容易过拟合。\n如果model偏差大，那么需要提更多的feature，或者换更复杂的model。如果model方差大，那么需要增加data(可以人工制造data)，或者加正则项，降低model复杂度。\n模型选择（model selection），注意这里是模型选择，可以通过交叉验证进行，以树模型为例，其实也就是对树深度等超参数进行选择，这里的超参数相当于模型。\n在看这个视频之前，我对高方差的理解是：假设现在有数据集D1，在D1上训练得到了函数${f_1^*}$，那么${f_1^*}$会在另一个数据集D2上表现很差。看完这个视频之后，发现我这样理解确实是正确的（因为高方差本身就是说模型在不同的数据集上训练得到的${f^*}$很不一样，也就是说在D1上表现很好的${f_1^*}$和在D2上表现很好的${f_2^*}$会很不一样，即${f_1^*}$会在另一个数据集D2上表现很差），但还是不够深入。\n还有就是，其实高方差模型并不是不好（DNN不就是很复杂的模型嘛），因为其bias很低，因此只要搞到足够的data，最后取平均，那么就会逼近真实的函数。但是在现实中，由于数据量有限，因此还是不会让模型复杂度太高，尤其是打比赛的时候。"},"title":"3.偏差与方差"},"/blog/li_ml_2017/4.%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/":{"data":{"adagrad#Adagrad":"Adagrad梯度下降的学习率$\\eta $既不能设置的太大，也不能太小。\n自适应的学习率：\n逐渐减小$\\eta $ 给不同的参数不同的学习率 Vanilla Gradient descent $$ w^{t+1}\\leftarrow w^{t}-\\eta ^{t}g^{t} $$ Adagrad $$ w^{t+1}\\leftarrow w^{t}-\\frac{\\eta ^{t}}{\\sigma ^{t}}g^{t} $$ 其中， $$ \\eta ^{t}=\\frac{\\eta }{\\sqrt{t+1}} $$ t为迭代次数\n$$g^{t}=\\frac{\\partial L(\\theta ^{t})}{\\partial w}$$\n$\\sigma ^{t}$为w之前所有微分值的均方根，即$\\sigma ^{t}=\\sqrt{\\frac{1}{t+1}\\sum_{i=0}^{t}\\left ( g^{i} \\right )^{2}} $ 化简之后，可以得到 $$ w^{t+1}\\leftarrow w^{t}-\\frac{\\eta }{\\sum_{i=0}^{t}\\left ( g^{i} \\right )^{2}} g^{t} $$\n图1 梯度下降\n从上图中可以看到，Vanilla Gradient descent的做法是，梯度$g^t$越大，那么梯度下降的步幅step：$\\eta^tg^t$也越大；而Adagrad的做法是：梯度$g^t$越大，梯度下降的步幅step：$\\frac{\\eta }{\\sum_{i=0}^{t}\\left ( g^{i} \\right )^{2}} g^{t}$的分子越大，但分母也会越大，即分子和分母对step的影响是相反的，如何解释呢？\n直观的解释是Adagrad中梯度下降的步幅step：$\\frac{\\eta }{\\sum_{i=0}^{t}\\left ( g^{i} \\right )^{2}} g^{t}$反映了第t轮的梯度值与之前轮次梯度值的反差程度。如下图所示，如果之前轮次的梯度一直很小，但第t轮的梯度值很大，那么step就会很大；如果之前轮次的梯度一直很大，但第t轮的梯度值很小，那么step就会很小。。。。。（所以呢？能说明啥？） 图2\n如下图3所示，以一元二次函数为例，$x_0$处的best step为$\\frac{\\left | 2ax_{0}+b \\right |}{2a}$，best step的分子为$\\left | 2ax_{0}+b \\right |$，其实是一元二次函数在$x_0$处的一阶微分；分母为$2a$，其实是一元二次函数在$x_0$处的二阶微分。也就是说， $$ best\\ step=\\frac{\\left | 一阶微分 \\right |}{二阶微分} $$ （但是这只是一元二次函数的情况，一阶微分/二阶微分 这种形式具有通用性吗？） 图3\n现在再来看Adagrad的表达式，step的分母$\\sum_{i=0}^{t}\\left ( g^{i} \\right )^{2}$其实相当于二阶微分，原因是，如果一个函数的二阶微分很大，那么对其一阶微分进行采样，求这些点的均方根，得到的结果也会大，即下图中右图绿色的函数；如果一个函数的二阶微分小，那么对其一阶微分进行采样，求这些点的均方根，得到的结果也会较小，如下图左图蓝色函数所示。\n也就是说，参数w(之前的)所有一阶微分值的均方根可以用来表示二阶微分。 图4","feature-scaling#Feature scaling":"","梯度下降#梯度下降":"以后提到梯度下降，就想象成“下山”（相当于有2个参数）。\n需要注意的是，每次更新参数后，损失函数不一定会下降！（跟学习率有关）\n梯度下降的本质是只考虑一阶项时的泰勒展开。 牛顿法考虑了2阶导数。\n为什么梯度下降可以使损失函数降低，因此梯度的方向是函数增长最快（一 阶导数）的方向，因此将参数朝着负梯度方向进行更新，可以使函数值降低。那么为什么梯度的方向是函数增长的方向呢，因为泰勒展开中的一阶项是+号（N阶项都是+号）。\n而且由于泰勒展开的前提是$\\Delta x$无限小，因此学习率$\\eta$也需要无限小（即将参数限制在一个很小的范围内），才能满足泰勒展开的条件。\n就像玩红警一样，只有单位走过的地方我们才知道那里的地形高低（LOSS的大小），如果我们一步步走的话，可能只能走到局部最小点。其他没走过的地方是战争迷雾，只要开了全图作弊器，我们才知道全局最小点的位置。这就和泰勒展开一个道理，泰勒展开的前提条件是$\\Delta x$无限小，也就是只能看到周围很小的区域，采用梯度下降的方式，可能只能走到局部最小点。\n也就是说，由于梯度下降的本质是泰勒展开，那么梯度下降的问题就在于泰勒展开的前提条件是$\\Delta x$无限小，所以就会造成无法得到全局最优解的问题。","随机梯度下降#随机梯度下降":"更新参数时不是用所有样本上的梯度，而是只用一个样本上的梯度来更新参数。"},"title":"4.梯度下降"},"/blog/li_ml_2017/5.%E5%88%86%E7%B1%BB%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/":{"data":{"":"如果把分类任务直接当成回归来做的话，那么会存在2个问题：\nregression会惩罚那些太正确的点（离分界面很远的那些点） 假设有3类，如果直接把1,2,3三个数字作为回归的目标时，那么1和2之间的距离、2和3之间的距离要比1和3之间的距离要近，而其实1，2，3作为3个类别是没有距离概念的。 其实分类问题（生成式模型）本质就是贝叶斯公式。 现在假设有2个类C1,C2，分类任务就是给定一个样本x，判断该样本属于C1还是C2，具体怎样判断呢？我们可以将这个问题转换为给定一个样本x，得到该样本属于C1和C2的概率，根据概率值判断是否属于这个类（如概率大于0.5）。根据贝叶斯公式，样本x属于C1类的概率为： $$ P(C_1|x)=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)} $$ 这个贝叶斯公式是生成式模型的本质！ 为了得到$P(C_1|x)$，就需要知道4个概率：2个先验概率$P(C_1)、P(C_2)$以及$P(x|C_1)$、$P(x|C_2)$，这也是为什么叫生成式模型的原因，因为一旦有了这4个概率分布，我们就可以根据这个概率分布自己生成数据。\n那么怎样得到这4个概率分布呢，2个先验概率$P(C_1)、P(C_2)$可以简单地根据C1和C2类的占比来得到，但是$P(x|C_1)$和$P(x|C_2)$如果直接用x在C1中出现的次数/C1的总样本数，那么x可能在C1中压根就没有出现过（x来自测试集嘛），因此我们需要估计出C1中x的概率分布（$P(x|C_1)$）和C2中x的概率分布（$P(x|C_2)$）。 做法是，先假设C1、C2中x都服从某一类分布，如M维高斯分布，然后通过极大似然法得到分布的参数。\n高斯分布的参数可以不一样，即均值向量$\\mathbf{u_1}$不等于 $\\mathbf{u_2}$（$\\mathbf{u_1}$和$\\mathbf{u_2}$为M维向量，M为特征个数），协方差矩阵$\\Sigma_1$也不等于$\\Sigma_2$ 高斯分布的参数也可以一样，但是肯定不能完全一样。比即均值向量$\\mathbf{u_1}$不等于 $\\mathbf{u_2}$，但协方差矩阵$\\Sigma_1$等于$\\Sigma_2$ 一般采用第二种方式，即让分布的某些参数相同，这样可以减少参数数量，且学习效果也可能变好。视频中将两个分布share相同的协方差矩阵后，分界面变成线性的了，分类效果也变好了，具体原因下面再解释。\n之前的视频中说到机器学习任务可以分为3个部分，model(function set)、loss function、找到最优参数，对于分类问题，\nmodel就是 $P(C_1|x)=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}$，如果P大于0.5，则属于C1类。 loss function损失函数其实就是极大似然中的似然函数。 找到最优参数就是极大似然法求得最优的参数。 当然也可以假设feature之间是相互独立（朴素贝叶斯） 的（即每一列feature都服从一个一维的高斯分布，即协方差矩阵非对角线位置全为0），但这样效果可能会变差，因为模型变简单了。\n当然也可以不假设服从高斯分布，比如某个feature取值为0和1，可以假设其服从伯努利分布。\n由贝叶斯后验概率公式得到sigmoid函数：\n图1\n当协方差矩阵$\\Sigma_1$和$\\Sigma_2$相等时，可以得到线性分界面。\n图2\n在生成式模型中，我们需要估计出N1,N2,u1,u2,$\\Sigma$，进而得到w和b，那么为什么不直接通过训练得到w和b呢？（生成式和判别式的区别）Logistic回归就是这么干的！\n也就是说，对于生成式模型来说，分界面为线性的原因在于：两个类别C1和C2下x的协方差矩阵相同（存在协方差矩阵的前提是两个类别C1和C2下x同时服从D维的高斯分布，若是其他分布，就不一定是协方差矩阵相同这个原因了）\n总结起来就2点：1.sigmoid函数本质是贝叶斯后验概率公式sigmoid函数可以由贝叶斯公式推导得到。 2.逻辑斯蒂回归中的线性分界面的本质是两个类别C1和C2下x的协方差矩阵相同。\nLogistic回归也是假设两个类别C1和C2下x同时服从D维的高斯分布吗？？？？？不是，LR是判别式模型，不需要假设数据分布，逻辑斯蒂回归中分界面是线性的就是单纯直接假设出来的，跟之前$\\Sigma_1$和$\\Sigma_2$相等时得到的分界面是线性的具有本质上的不同，因为一个是判别式模型，一个是生成式模型。"},"title":"5.分类（生成式模型）"},"/blog/li_ml_2017/6.%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/":{"data":{"":"逻辑斯蒂回归假设二分类的分界面为线性的，即 $$ P(C_1|x)=\\sigma _{w,b}(z)=\\sigma(\\sum w_ix_i+b)=f_{w,b}(x) $$ 如何选择最优的w和b呢，还是用极大似然法。\n对似然函数取对数，取负号，并根据类别的取值0，1，就可以由似然函数得到交叉熵损失函数，如下图所示。 图1\n交叉熵衡量的是两个分布的相似程度，如果两个分布完全一致，则交叉熵为0。\n这也为什么要用0，1表示类别，因为概率的取值范围是[0,1]，只有用0，1表示类别，才能将似然函数写成交叉熵损失函数的形式。\n有了损失函数后，用梯度下降法求出w,b。\nLR和线性回归和对比如下图所示。 图2 可以看出，LR梯度下降中$w_i$更新式子的形式和线性回归一样。\nLR如果用平方损失函数，那么从w的求导表达式可以看出，当真实值和预测值相差很大时，梯度仍然很小，不符合逻辑。\n总结一下，LR（判别式模型）其实和生成式模型（前提是如果是高斯分布，二者share同一个协方差矩阵）的model（function set）是一样的，都是 $$ P(C_1|x)=\\sigma _{w,b}(z)=\\sigma(\\sum w_ix_i+b) $$ 二者的区别在于，LR没有任何前提假设，直接求解参数w,b；而生成式模型需要假设样本在每个类别下服从某种分布，然后求出分布的参数（如$u_1,u_2,\\Sigma$），然后由这些分布的参数再得到w,b。因此，二者找出的w,b通常是不同的。\n通常情况下，判别式模型的效果会比生成式模型的效果好一些。生成式模型会“脑补”一些数据集中没有发生的情况，会对noise鲁棒一些。\n对于多分类问题，可以将sigmoid函数换为softmax函数，损失函数仍然用多变量的交叉熵损失函数。\nLR无法解决线性不可分问题，这是模型本身的问题，可以通过feature转换来解决。\n如果将多个LR串接起来，其实每个LR相当于做了不同的feature转换，即一个神经元，这些LR的组合其实就是神经网络。"},"title":"6.逻辑斯蒂回归(判别式模型)"},"/blog/li_ml_2017/7.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/":{"data":{"":"神经网络相当自动做特征工程，那么之前特征工程的工作就转变成设计神经网络的结构了。\n其实任何一个连续函数都可以被只包含一层hidden layer的神经网络所表示，只要这个hidden layer中包含足够的神经元，也就是fat network，那么deep有啥用。。。"},"title":"7.深度学习简介"},"/blog/li_ml_2017/8.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/":{"data":{"":"图1 误差反向传播本质就是链式求导法则，如上图所示。ｓ可以看成第ｌ层中的某一个神经元，ｘ，ｙ可以看成第ｌ＋１层中和ｓ相连接的两个神经元，那么相当于x和ｙ都是关于ｓ的函数 （ｓ会影响到ｘ和ｙ，因为相连嘛）。\n那么损失函数Ｌ对ｓ的导数，就等于Ｌ对ｘ的导数乘以ｘ对ｓ的导数＋Ｌ对ｙ的导数乘以ｙ对ｓ的导数。"},"title":"8.反向传播"},"/blog/li_ml_2017/9.hello-world-keras/":{"data":{"":"用mini-batch的原因：\n如果用随机梯度下降的话，需要对每一个样本单独计算LOSS，而如果是mini-batch的话，可以同时对多个样本计算LOSS，也就是可以写成矩阵相乘的形式，GPU在做矩阵运算时可以并行计算。 如果用批梯度下降的话，那么缺少随机性，很容易迭代几次就掉到鞍点或者局部最小点。 "},"title":"9.hello world-Keras"},"/blog/matrix/%E7%90%86%E8%A7%A3n%E7%BB%B4%E6%95%B0%E7%BB%84/":{"data":{"":"n维数组中的每一个元素都是一个n-1维数组，假设共m个元素，这m个元素在n-1维空间之外的另一维度有序排列，构成了这个n维数组。\n以3维数组为例，3维数组中的每一个元素都是一个2维数组，每一个2维数组可以看成是站在地面上的小人组成的阅兵方阵，假设有m个方阵，每个方阵，有20排，10列，（即这个2维数组大小[20,10]）如果这m个方阵都站成地面上并有序排列（下一个方阵补在上一个方阵最后一排后面），那么这m个方阵组成的大方阵仍然是一个2维数组，数组大小为[(m*20),10]。但是如果下一个方阵补在上一个方阵的头顶上（地面（XY平面）之外的另一维度（高度，Z轴）），那么这m个方阵组成的大方阵是一个3维数组，数组大小为[m,20,10]"},"title":"理解n维数组"},"/blog/matrix/%E7%9F%A9%E9%98%B5%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/":{"data":{"":"变换本质上是函数\n先说结论：矩阵的本质是对空间的线性变换。\n什么是线性变换呢，先来看什么是变换。变换本质上是函数，函数的作用是接收输入内容，并输出对应结果。对于变换来说，其输入是一个向量，输出是经变换后的向量，这个过程相当于将输入向量移动到输出向量位置。那么，对于一个变换来说，这个变换可以将空间中的每一个向量都移动到对应输出向量的位置，如果把向量看作是一个点的话，那么这个变换可以把空间中的所有点移动到其他点的位置。\n小结：\n变换的本质是函数，是空间中的点的函数，这个函数将空间中的所有点移动到其他点的位置（本来我在荣成，变换后到了合肥），其实就是一种操纵空间的手段。\n那么什么是线性变换呢？如果一个变换具有以下2个性质，那么这个变换就是线性的：\n直线在变换之后仍然是直线，不能有弯曲。 原点必须保持固定。 那么怎样来表示这个线性变换呢？ 以二维空间为例，只需要记录二维空间的两个基向量（二维空间有2个，三维空间有3个…N维空间有N个）$\\hat{\\mathbf{i}}(1，0)$和$\\hat{\\mathbf{j}}(0,1)$变换后的位置，其他向量都会随之而动。 也就是说，任一向量${\\mathbf{v}}$是$\\hat{\\mathbf{i}}$和$\\hat{\\mathbf{j}}$的一个特定线性组合，那么由于线性变换的2个性质，变换后的向量${\\mathbf{v}}$也是变换后的$\\hat{\\mathbf{i}}$和$\\hat{\\mathbf{j}}$的同样的线性组合。\n小结： 一个二维空间的线性变换可以由4个数字来确定，这4个数字是原始空间中的两个基向量$\\hat{\\mathbf{i}}(1，0)$和$\\hat{\\mathbf{j}}(0,1)$变换后的位置坐标。\n如果把这4个数字包装在一个2*2的格子中， 即2*2的矩阵。这个矩阵的第一列即变换后的$\\hat{\\mathbf{i}}$的坐标，第二列即变换后的$\\hat{\\mathbf{j}}$的坐标，如下图所示。\n也就是说，矩阵的N个列可以看成是线性变换后的N个基向量!\n实际上，矩阵只是线性变换的一种记号，一种表示方式。每当看到一个矩阵时，你都可以把他解读为对空间的一种特定变换。"},"title":"矩阵与线性变换"},"/blog/matrix/markdown/":{"data":{"":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files.","basic-syntax#Basic Syntax":"Headings # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 ","heading-2#Heading 2":"Heading 3 Heading 4 Heading 5 Heading 6 Emphasis *This text will be italic* _This will also be italic_ **This text will be bold** __This will also be bold__ _You **can** combine them_ This text will be italic\nThis will also be italic\nThis text will be bold\nThis will also be bold\nYou can combine them\nLists Unordered * Item 1 * Item 2 * Item 2a * Item 2b Item 1 Item 2 Item 2a Item 2b Ordered 1. Item 1 2. Item 2 3. Item 3 1. Item 3a 2. Item 3b Images ![GitHub Logo](https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png) Links [Hugo](https://gohugo.io) Hugo\nBlockquotes As Newton said: \u003e If I have seen further it is by standing on the shoulders of Giants. If I have seen further it is by standing on the shoulders of Giants.\nInline Code Inline `code` has `back-ticks around` it. Inline code has back-ticks around it.\nCode Blocks Syntax Highlighting ```go func main() { fmt.Println(\"Hello World\") } ``` func main() { fmt.Println(\"Hello World\") } Tables | Syntax | Description | | --------- | ----------- | | Header | Title | | Paragraph | Text | Syntax Description Header Title Paragraph Text ","references#References":" Markdown Syntax Hugo Markdown "},"title":"Markdown Syntax Guide"},"/docs/":{"data":{"":"This is a demo of the theme’s documentation layout.","hello-world#Hello, World!":"main.gopackage main import \"fmt\" func main() { fmt.Println(\"Hello, World!\") } "},"title":"Documentation"},"/docs/first-page/":{"data":{"":"A simple demo page."},"title":"Demo Page"},"/docs/folder/":{"data":{"":"Pages can be organized into folders."},"title":"Folder"},"/docs/folder/leaf/":{"data":{"":"This page is under a folder."},"title":"Leaf Page"}}